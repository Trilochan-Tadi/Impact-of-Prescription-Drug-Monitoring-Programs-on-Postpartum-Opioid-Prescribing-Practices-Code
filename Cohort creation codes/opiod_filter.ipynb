{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import time\n",
    "from pandas._libs.tslibs.parsing import DateParseError\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndc_file_path = r\"\\\\chcdfiles.uthouston.edu\\extract\\vivas_postpartum\\chelsea\\datalake\\final_codes\\ndc_opioids_codes.csv\"\n",
    "opioid_codes_df = pd.read_csv(ndc_file_path, dtype={\"ndc\": str})\n",
    "ndc_codes = opioid_codes_df['ndc'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_path = r\"\\\\chcdfiles.uthouston.edu\\extract\\vivas_postpartum\\chelsea\\datalake\\New_cohort\\New_filters\\Cohort\\42day_post_delivery_filter.csv\"\n",
    "output_file_path = r\"\\\\chcdfiles.uthouston.edu\\extract\\vivas_postpartum\\chelsea\\datalake\\New_cohort\\New_filters\\Cohort\\42_opioid_info.csv\"  \n",
    "ndc_codes_path =  r\"\\\\chcdfiles.uthouston.edu\\extract\\vivas_postpartum\\chelsea\\datalake\\final_codes\\ndc_opioids_codes.csv\"  \n",
    "\n",
    "# Load NDC codes for opioid prescriptions\n",
    "ndc_codes_df = pd.read_csv(ndc_codes_path)\n",
    "ndc_codes = ndc_codes_df['ndc'].tolist()\n",
    "\n",
    "# Function to identify opioid dates and relevant information\n",
    "def identify_opioids_dates(dataframe: pd.DataFrame, ndc_codes: list,\n",
    "                           pat_id_p: str = 'pat_id_p',\n",
    "                           ndc: str = \"ndc\", \n",
    "                           from_dt: str = \"from_dt\",\n",
    "                           quan: str = \"quan\",\n",
    "                           dayssup: str = \"dayssup\",\n",
    "                           opioid_dts: str = \"opioid_dates\",\n",
    "                           presc_opioid: str = \"presc_opioid\",\n",
    "                           op_dayssup: str = \"op_dayssup\",\n",
    "                           op_quan: str = \"op_quan\"):\n",
    "                           \n",
    "    grouped_df = dataframe.groupby(by=pat_id_p)\n",
    "    result_df = pd.DataFrame()\n",
    "    \n",
    "    for pat_id, group in grouped_df:\n",
    "        opioid_dates = group.loc[group[ndc].isin(ndc_codes), from_dt]\n",
    "        opioid_dates = opioid_dates.tolist() if not opioid_dates.empty else []\n",
    "        \n",
    "        opioid_presc = group.loc[group[ndc].isin(ndc_codes), ndc]\n",
    "        opioid_presc = opioid_presc.tolist() if not opioid_presc.empty else []\n",
    "        \n",
    "        opioid_quan = group.loc[group[ndc].isin(ndc_codes), quan]\n",
    "        opioid_quan = opioid_quan.tolist() if not opioid_quan.empty else []\n",
    "        \n",
    "        opioid_ds = group.loc[group[ndc].isin(ndc_codes), dayssup]\n",
    "        opioid_ds = opioid_ds.tolist() if not opioid_ds.empty else []\n",
    "\n",
    "        # Create a DataFrame for the current patient's opioid information\n",
    "        patient_result_df = pd.DataFrame({\n",
    "            pat_id_p: [pat_id], \n",
    "            opioid_dts: [opioid_dates], \n",
    "            presc_opioid: [opioid_presc], \n",
    "            op_dayssup: [opioid_ds], \n",
    "            op_quan: [opioid_quan]\n",
    "        })\n",
    "        \n",
    "        result_df = pd.concat([result_df, patient_result_df])\n",
    "    \n",
    "    # Merge back with the original dataframe\n",
    "    result_df = pd.merge(dataframe, result_df, on=pat_id_p, how='left')\n",
    "\n",
    "    return result_df\n",
    "\n",
    "# Function to process chunks and create opioid dates\n",
    "def process_chunks_and_create_opioid_dates(input_file, output_file, ndc_codes, chunk_size=5000):\n",
    "    processed_patients = 0\n",
    "    processed_chunks = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    with pd.read_csv(input_file, chunksize=chunk_size) as reader:\n",
    "        for chunk_idx, chunk_df in enumerate(reader):\n",
    "            print(f\"\\nProcessing chunk {chunk_idx + 1}\")\n",
    "            try:\n",
    "                # Identify opioid dates\n",
    "                chunk_df_with_opioid_dates = identify_opioids_dates(chunk_df, ndc_codes)\n",
    "\n",
    "                # Append results to the output file\n",
    "                chunk_df_with_opioid_dates.to_csv(output_file, mode='a', index=False, header=(chunk_idx == 0))\n",
    "\n",
    "                # Track the number of processed patients\n",
    "                processed_patients += len(chunk_df_with_opioid_dates)\n",
    "                processed_chunks += 1\n",
    "\n",
    "                print(f\"Processed patients: {processed_patients}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing chunk {chunk_idx + 1}: {e}\")\n",
    "                continue\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"Total processing time: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "# Process the file in chunks and create opioid dates\n",
    "process_chunks_and_create_opioid_dates(input_file_path, output_file_path, ndc_codes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file paths\n",
    "input_file_path =r\"\\\\chcdfiles.uthouston.edu\\extract\\vivas_postpartum\\chelsea\\datalake\\New_cohort\\New_filters\\Cohort\\42_opioid_info.csv\"  \n",
    "output_file_path = r\"\\\\chcdfiles.uthouston.edu\\extract\\vivas_postpartum\\chelsea\\datalake\\New_cohort\\New_filters\\Cohort\\42_opioid_in_pregnancy.csv\"\n",
    "chunk_size = 100000\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "# Function to check opioid use in the 90 days prior to delivery\n",
    "def check_opioid_in_pregnancy(\n",
    "    chunk_df,\n",
    "    pat_id_p='pat_id_p',\n",
    "    delivery_dt='delivery_dt',\n",
    "    opioid_dates_col='opioid_dates'\n",
    "):\n",
    "    # 1 = exposed in 90 days pre-delivery, 0 = not exposed\n",
    "    chunk_df['opioid_exposed_90d_pre_delivery'] = 0\n",
    "\n",
    "    for pat_id, group_df in chunk_df.groupby(pat_id_p):\n",
    "        delivery_date = pd.to_datetime(\n",
    "            group_df[delivery_dt].iloc[0],\n",
    "            errors='coerce'\n",
    "        )\n",
    "\n",
    "        if pd.isna(delivery_date):\n",
    "            continue\n",
    "\n",
    "        start = delivery_date - pd.Timedelta(days=90)\n",
    "        end = delivery_date  # delivery day not counted as \"pre\"\n",
    "\n",
    "        opioid_dates = group_df[opioid_dates_col].iloc[0]\n",
    "        if pd.isna(opioid_dates):\n",
    "            continue\n",
    "\n",
    "        opioid_dates = [\n",
    "            pd.to_datetime(d.strip(\"[Timestramp('')]\"), errors='coerce')\n",
    "            for d in opioid_dates.split(\", \")\n",
    "        ]\n",
    "\n",
    "        if any(pd.notna(d) and (start <= d < end) for d in opioid_dates):\n",
    "            chunk_df.loc[\n",
    "                group_df.index,\n",
    "                'opioid_exposed_90d_pre_delivery'\n",
    "            ] = 1\n",
    "\n",
    "    return chunk_df\n",
    "\n",
    "# Function to check opioid use in the 7 days after delivery\n",
    "def check_opioid_after_delivery(chunk_df, pat_id_p='pat_id_p', delivery_dt='delivery_dt', opioid_dates_col='opioid_dates'):\n",
    "    chunk_df['opioid_after_delivery'] = 'No'  # Default to \"No\" (no opioid use after delivery period)\n",
    "\n",
    "    for pat_id, group_df in chunk_df.groupby(pat_id_p):\n",
    "        delivery_date = pd.to_datetime(group_df[delivery_dt].iloc[0], errors='coerce')\n",
    "        \n",
    "        if pd.notna(delivery_date):\n",
    "            delivery_dt_7 = delivery_date + pd.Timedelta(days=7)\n",
    "\n",
    "            # Parse opioid dates\n",
    "            opioid_dates = group_df[opioid_dates_col].iloc[0]\n",
    "            if not pd.isna(opioid_dates):\n",
    "                opioid_dates = [pd.to_datetime(date.strip(\"[Timestramp('')]\"), errors='coerce') for date in opioid_dates.split(\", \")]\n",
    "\n",
    "                # If any opioid use is found within 0 to 7 days, set \"Yes\"\n",
    "                # If any opioid use is found beyond 7 days, set \"No\"\n",
    "                if any(pd.notna(date) and delivery_date <= date <= delivery_dt_7 for date in opioid_dates):\n",
    "                    chunk_df.loc[group_df.index, 'opioid_after_delivery'] = 'Yes'\n",
    "                elif any(pd.notna(date) and date > delivery_dt_7 for date in opioid_dates):\n",
    "                    chunk_df.loc[group_df.index, 'opioid_after_delivery'] = 'No'\n",
    "                    \n",
    "    return chunk_df\n",
    "\n",
    "# Function to process each chunk and filter based on both conditions\n",
    "def process_chunk(chunk_df):\n",
    "    # Create binary pre-90 exposure flag\n",
    "    chunk_df = check_opioid_in_pregnancy(chunk_df)\n",
    "\n",
    "    # Create postpartum 7-day Yes/No flag\n",
    "    chunk_df = check_opioid_after_delivery(chunk_df)\n",
    "\n",
    "    # KEEP ALL pregnancies, only keep those with opioid within 7 days after delivery\n",
    "    filtered_chunk = chunk_df[chunk_df['opioid_after_delivery'] == 'Yes']\n",
    "\n",
    "    return filtered_chunk\n",
    "\n",
    "# Function to process each chunk and track progress\n",
    "def process_data_sequentially():\n",
    "    total_chunks = sum(1 for _ in pd.read_csv(input_file_path, chunksize=chunk_size))\n",
    "    processed_chunks = 0\n",
    "\n",
    "    with pd.read_csv(input_file_path, chunksize=chunk_size) as reader:\n",
    "        for chunk_idx, chunk_df in enumerate(reader):\n",
    "            print(f\"\\nProcessing chunk {chunk_idx + 1} of {total_chunks}\")\n",
    "            \n",
    "            # Process the chunk\n",
    "            filtered_chunk = process_chunk(chunk_df)\n",
    "            \n",
    "            # Append results to the output file\n",
    "            mode = 'a' if chunk_idx > 0 else 'w'\n",
    "            filtered_chunk.to_csv(output_file_path, mode=mode, index=False, header=(chunk_idx == 0))\n",
    "\n",
    "            # Track progress\n",
    "            processed_chunks += 1\n",
    "            elapsed_time = time.time() - start_time\n",
    "            estimated_time_remaining = (total_chunks - processed_chunks) * (elapsed_time / processed_chunks)\n",
    "\n",
    "            # Convert estimated time remaining to HH:MM:SS\n",
    "            hours, rem = divmod(estimated_time_remaining, 3600)\n",
    "            minutes, seconds = divmod(rem, 60)\n",
    "            progress_percentage = (processed_chunks / total_chunks) * 100\n",
    "\n",
    "            # Display progress\n",
    "            print(f\"Chunk {chunk_idx + 1}/{total_chunks} processed\")\n",
    "            print(f\"Total elapsed time: {elapsed_time:.2f} seconds\")\n",
    "            print(f\"Estimated time remaining: {int(hours):02}:{int(minutes):02}:{int(seconds):02}\")\n",
    "            print(f\"Progress: {progress_percentage:.2f}%\")\n",
    "\n",
    "    total_elapsed_time = time.time() - start_time\n",
    "    print(f\"\\nTotal processing time: {total_elapsed_time:.2f} seconds\")\n",
    "\n",
    "# Run the main processing function\n",
    "process_data_sequentially()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "03eb0f413ac8a21b237757e6c01d7f64cf7b4655952acb378d042bd2774cacd1"
  },
  "kernelspec": {
   "display_name": "Python 3.11.7 ('Isabel')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
